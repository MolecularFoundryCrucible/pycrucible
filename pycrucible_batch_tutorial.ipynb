{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PycrucibleBatch Tutorial: Hierarchical Sample Management\n",
    "\n",
    "This tutorial demonstrates how to use the pycrucible client to manage batches of samples using a hierarchical approach where:\n",
    "- A \"batch\" is created as a parent sample\n",
    "- Individual samples are created as children of the batch\n",
    "- Datasets can be associated at both batch and individual sample levels\n",
    "- Data can be retrieved and downloaded by batch\n",
    "\n",
    "## Prerequisites\n",
    "- pycrucible package installed\n",
    "- Valid Crucible API credentials\n",
    "- Sample data files (we'll create some for demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pycrucible import CrucibleClient\n",
    "import uuid\n",
    "from typing import List, Dict\n",
    "\n",
    "# Configuration - Update these with your credentials\n",
    "API_URL = \"https://your-crucible-api.com\"  # Replace with your API URL\n",
    "API_KEY = \"your-api-key-here\"  # Replace with your API key\n",
    "\n",
    "# Initialize the client\n",
    "client = CrucibleClient(API_URL, API_KEY)\n",
    "print(\"Crucible client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Sample Data Files\n",
    "\n",
    "First, let's create some sample files to work with in our tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for our tutorial files\n",
    "os.makedirs(\"tutorial_data\", exist_ok=True)\n",
    "\n",
    "# Create a batch-level data file (e.g., experimental protocol)\n",
    "batch_data = {\n",
    "    \"experiment_name\": \"Protein Expression Analysis\",\n",
    "    \"protocol_version\": \"2.1\",\n",
    "    \"date\": datetime.now().isoformat(),\n",
    "    \"conditions\": {\n",
    "        \"temperature\": \"37°C\",\n",
    "        \"pH\": 7.4,\n",
    "        \"buffer\": \"PBS\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"tutorial_data/batch_protocol.json\", \"w\") as f:\n",
    "    json.dump(batch_data, f, indent=2)\n",
    "\n",
    "# Create individual sample image files (simulated microscopy images)\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Generate 5 sample \"microscopy\" images\n",
    "sample_names = [\"Sample_A1\", \"Sample_A2\", \"Sample_B1\", \"Sample_B2\", \"Sample_C1\"]\n",
    "\n",
    "for i, sample_name in enumerate(sample_names):\n",
    "    # Create a simple pattern image (simulating microscopy data)\n",
    "    data = np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8)\n",
    "    # Add some structure to make it look more realistic\n",
    "    center = (128, 128)\n",
    "    y, x = np.ogrid[:256, :256]\n",
    "    mask = (x - center[0])**2 + (y - center[1])**2 < 60**2\n",
    "    data[mask] = [100 + i*30, 150, 200]  # Different colors for each sample\n",
    "    \n",
    "    img = Image.fromarray(data)\n",
    "    img.save(f\"tutorial_data/{sample_name}_microscopy.png\")\n",
    "\n",
    "print(f\"Created tutorial data files:\")\n",
    "for file in os.listdir(\"tutorial_data\"):\n",
    "    print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create a Batch (Parent Sample)\n",
    "\n",
    "We'll create a parent sample that represents our batch. This sample will contain metadata about the entire batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a unique batch ID\n",
    "batch_id = f\"BATCH_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{str(uuid.uuid4())[:8]}\"\n",
    "print(f\"Creating batch with ID: {batch_id}\")\n",
    "\n",
    "# Create the batch sample\n",
    "batch_sample_response = client.add_sample(\n",
    "    sample_name=batch_id,\n",
    "    sample_description=f\"Batch sample for protein expression analysis experiment. Contains {len(sample_names)} individual samples.\",\n",
    "    sample_creation_date=datetime.now().isoformat(),\n",
    "    sample_owner_orcid=\"0000-0000-0000-0000\"  # Replace with actual ORCID\n",
    ")\n",
    "\n",
    "print(f\"Batch sample created:\")\n",
    "print(f\"Status Code: {batch_sample_response.status_code}\")\n",
    "if batch_sample_response.status_code == 201:\n",
    "    batch_sample = batch_sample_response.json()\n",
    "    batch_sample_id = batch_sample['id']\n",
    "    print(f\"Batch Sample ID: {batch_sample_id}\")\n",
    "    print(f\"Batch Sample: {json.dumps(batch_sample, indent=2)}\")\n",
    "else:\n",
    "    print(f\"Error: {batch_sample_response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Individual Samples (Children of Batch)\n",
    "\n",
    "Now we'll create individual samples that are linked to our batch through a custom parent_sample_id field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's extend the add_sample method to support custom fields\n",
    "def add_sample_with_parent(client, sample_name, sample_description, parent_sample_id=None, \n",
    "                          sample_creation_date=None, sample_owner_orcid=None, owner_id=None):\n",
    "    \"\"\"Extended sample creation that supports parent_sample_id\"\"\"\n",
    "    sample_info = {\n",
    "        \"sample_name\": sample_name, \n",
    "        \"owner_orcid\": sample_owner_orcid,\n",
    "        \"owner_user_id\": owner_id,\n",
    "        \"description\": sample_description,\n",
    "        \"date_created\": sample_creation_date\n",
    "    }\n",
    "    \n",
    "    # Add parent relationship if specified\n",
    "    if parent_sample_id:\n",
    "        sample_info[\"parent_sample_id\"] = parent_sample_id\n",
    "    \n",
    "    import requests\n",
    "    response = requests.post(f\"{client.api_url}/samples\", headers=client.headers, json=sample_info)\n",
    "    return response\n",
    "\n",
    "# Create individual samples linked to our batch\n",
    "individual_samples = []\n",
    "\n",
    "for sample_name in sample_names:\n",
    "    print(f\"Creating sample: {sample_name}\")\n",
    "    \n",
    "    sample_response = add_sample_with_parent(\n",
    "        client,\n",
    "        sample_name=sample_name,\n",
    "        sample_description=f\"Individual sample from batch {batch_id}. Microscopy analysis of protein expression.\",\n",
    "        parent_sample_id=batch_sample_id,\n",
    "        sample_creation_date=datetime.now().isoformat(),\n",
    "        sample_owner_orcid=\"0000-0000-0000-0000\"  # Replace with actual ORCID\n",
    "    )\n",
    "    \n",
    "    if sample_response.status_code == 201:\n",
    "        sample_data = sample_response.json()\n",
    "        individual_samples.append(sample_data)\n",
    "        print(f\"  ✓ Created: {sample_data['sample_name']} (ID: {sample_data['id']})\")\n",
    "    else:\n",
    "        print(f\"  ✗ Failed: {sample_response.status_code} - {sample_response.text}\")\n",
    "\n",
    "print(f\"\\nCreated {len(individual_samples)} individual samples linked to batch {batch_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Add Batch-Level Dataset\n",
    "\n",
    "Upload the experimental protocol file as a dataset associated with the entire batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset for the batch-level protocol file\n",
    "batch_dataset = client.create_dataset(\n",
    "    dataset_name=f\"{batch_id}_Protocol\",\n",
    "    unique_id=f\"{batch_id}_protocol_{str(uuid.uuid4())[:8]}\",\n",
    "    public=False,\n",
    "    owner_orcid=\"0000-0000-0000-0000\",  # Replace with actual ORCID\n",
    "    measurement=\"experimental_protocol\",\n",
    "    session_name=batch_id,\n",
    "    data_format=\"json\",\n",
    "    scientific_metadata={\n",
    "        \"experiment_type\": \"protein_expression\",\n",
    "        \"batch_id\": batch_id,\n",
    "        \"protocol_version\": \"2.1\",\n",
    "        \"sample_count\": len(sample_names)\n",
    "    },\n",
    "    keywords=[\"protocol\", \"batch\", \"protein_expression\"]\n",
    ")\n",
    "\n",
    "print(f\"Batch dataset created: {batch_dataset['unique_id']}\")\n",
    "\n",
    "# Upload the protocol file\n",
    "upload_result = client.upload_dataset(batch_dataset['unique_id'], \"tutorial_data/batch_protocol.json\")\n",
    "print(f\"Protocol file uploaded: {upload_result}\")\n",
    "\n",
    "# Associate the batch dataset with the batch sample\n",
    "batch_link_response = client.add_dataset_to_sample(batch_sample_id, batch_dataset['unique_id'])\n",
    "print(f\"Batch dataset linked to batch sample: {batch_link_response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Individual Datasets for Each Sample\n",
    "\n",
    "Upload microscopy images as individual datasets for each sample in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_datasets = []\n",
    "\n",
    "for i, (sample_data, sample_name) in enumerate(zip(individual_samples, sample_names)):\n",
    "    print(f\"Creating dataset for {sample_name}\")\n",
    "    \n",
    "    # Create dataset for the microscopy image\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=f\"{sample_name}_Microscopy\",\n",
    "        unique_id=f\"{sample_name}_microscopy_{str(uuid.uuid4())[:8]}\",\n",
    "        public=False,\n",
    "        owner_orcid=\"0000-0000-0000-0000\",  # Replace with actual ORCID\n",
    "        measurement=\"microscopy\",\n",
    "        session_name=batch_id,\n",
    "        data_format=\"png\",\n",
    "        scientific_metadata={\n",
    "            \"experiment_type\": \"protein_expression\",\n",
    "            \"batch_id\": batch_id,\n",
    "            \"sample_name\": sample_name,\n",
    "            \"sample_position\": f\"Position_{i+1}\",\n",
    "            \"imaging_modality\": \"fluorescence_microscopy\",\n",
    "            \"magnification\": \"40x\",\n",
    "            \"exposure_time_ms\": 100\n",
    "        },\n",
    "        keywords=[\"microscopy\", \"protein_expression\", batch_id, sample_name]\n",
    "    )\n",
    "    \n",
    "    # Upload the microscopy image\n",
    "    image_file = f\"tutorial_data/{sample_name}_microscopy.png\"\n",
    "    upload_result = client.upload_dataset(dataset['unique_id'], image_file)\n",
    "    print(f\"  Uploaded: {image_file}\")\n",
    "    \n",
    "    # Associate the dataset with the individual sample\n",
    "    link_response = client.add_dataset_to_sample(sample_data['id'], dataset['unique_id'])\n",
    "    print(f\"  Linked to sample: {link_response.status_code}\")\n",
    "    \n",
    "    individual_datasets.append(dataset)\n",
    "    print(f\"  ✓ Dataset created: {dataset['unique_id']}\")\n",
    "\n",
    "print(f\"\\nCreated {len(individual_datasets)} individual datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Add Additional Metadata to Samples\n",
    "\n",
    "Demonstrate how to add custom metadata to individual samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add scientific metadata to individual datasets (since samples don't support metadata directly)\n",
    "sample_metadata_examples = [\n",
    "    {\"treatment\": \"control\", \"cell_density\": 1.2e6, \"viability\": 95.2},\n",
    "    {\"treatment\": \"treatment_A\", \"cell_density\": 1.1e6, \"viability\": 92.8},\n",
    "    {\"treatment\": \"treatment_B\", \"cell_density\": 1.3e6, \"viability\": 88.5},\n",
    "    {\"treatment\": \"control\", \"cell_density\": 1.2e6, \"viability\": 96.1},\n",
    "    {\"treatment\": \"treatment_C\", \"cell_density\": 1.0e6, \"viability\": 85.3}\n",
    "]\n",
    "\n",
    "for i, (dataset, metadata) in enumerate(zip(individual_datasets, sample_metadata_examples)):\n",
    "    # Get current metadata and add our custom fields\n",
    "    current_metadata = client.get_scientific_metadata(dataset['unique_id'])\n",
    "    updated_metadata = {**current_metadata, **metadata}\n",
    "    \n",
    "    # Update the scientific metadata\n",
    "    update_result = client.update_scientific_metadata(dataset['unique_id'], updated_metadata)\n",
    "    print(f\"Updated metadata for {sample_names[i]}: {metadata}\")\n",
    "\n",
    "print(\"\\nAll sample metadata updated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Query and Retrieve Batch Data\n",
    "\n",
    "Demonstrate how to find all data associated with a batch using our hierarchical structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_info(client, batch_id):\n",
    "    \"\"\"Retrieve all information for a batch\"\"\"\n",
    "    \n",
    "    # Find the batch sample\n",
    "    batch_samples = client.list_samples(sample_name=batch_id)\n",
    "    if batch_samples.status_code != 200 or not batch_samples.json():\n",
    "        print(f\"Batch {batch_id} not found\")\n",
    "        return None\n",
    "    \n",
    "    batch_sample = batch_samples.json()[0]\n",
    "    batch_sample_id = batch_sample['id']\n",
    "    \n",
    "    print(f\"Found batch sample: {batch_sample['sample_name']} (ID: {batch_sample_id})\")\n",
    "    \n",
    "    # Find all child samples (this would need API support for parent_sample_id queries)\n",
    "    # For now, we'll use our stored data\n",
    "    child_samples = individual_samples  # In practice, you'd query by parent_sample_id\n",
    "    \n",
    "    print(f\"\\nChild samples ({len(child_samples)}):\")\n",
    "    for sample in child_samples:\n",
    "        print(f\"  - {sample['sample_name']} (ID: {sample['id']})\")\n",
    "    \n",
    "    # Get datasets for batch and all child samples\n",
    "    all_sample_ids = [batch_sample_id] + [s['id'] for s in child_samples]\n",
    "    \n",
    "    batch_datasets = []\n",
    "    \n",
    "    # Note: This would require API endpoints to get datasets by sample ID\n",
    "    # For now, we'll use our stored dataset information\n",
    "    batch_datasets = [batch_dataset] + individual_datasets\n",
    "    \n",
    "    print(f\"\\nAssociated datasets ({len(batch_datasets)}):\")\n",
    "    for dataset in batch_datasets:\n",
    "        print(f\"  - {dataset['dataset_name']} ({dataset['unique_id']})\")\n",
    "    \n",
    "    return {\n",
    "        'batch_sample': batch_sample,\n",
    "        'child_samples': child_samples,\n",
    "        'datasets': batch_datasets\n",
    "    }\n",
    "\n",
    "# Test the batch retrieval\n",
    "batch_info = get_batch_info(client, batch_id)\n",
    "print(f\"\\nBatch {batch_id} contains:\")\n",
    "print(f\"  - 1 batch sample\")\n",
    "print(f\"  - {len(batch_info['child_samples'])} individual samples\")\n",
    "print(f\"  - {len(batch_info['datasets'])} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Download All Batch Data\n",
    "\n",
    "Download all datasets associated with a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_batch_data(client, batch_info, download_dir=\"batch_downloads\"):\n",
    "    \"\"\"Download all datasets for a batch\"\"\"\n",
    "    \n",
    "    batch_id = batch_info['batch_sample']['sample_name']\n",
    "    batch_download_dir = os.path.join(download_dir, batch_id)\n",
    "    os.makedirs(batch_download_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Downloading batch data to: {batch_download_dir}\")\n",
    "    \n",
    "    downloaded_files = []\n",
    "    \n",
    "    for dataset in batch_info['datasets']:\n",
    "        dataset_id = dataset['unique_id']\n",
    "        dataset_name = dataset['dataset_name']\n",
    "        \n",
    "        print(f\"\\nProcessing dataset: {dataset_name}\")\n",
    "        \n",
    "        # Get dataset details to find file information\n",
    "        dataset_details = client.get_dataset(dataset_id)\n",
    "        \n",
    "        # This is a simplified approach - in practice you'd need to:\n",
    "        # 1. Get list of files in the dataset\n",
    "        # 2. Download each file\n",
    "        \n",
    "        # For our tutorial, we know the file structure\n",
    "        if \"protocol\" in dataset_name.lower():\n",
    "            filename = \"batch_protocol.json\"\n",
    "        else:\n",
    "            # Extract sample name from dataset name\n",
    "            sample_name = dataset_name.split('_Microscopy')[0]\n",
    "            filename = f\"{sample_name}_microscopy.png\"\n",
    "        \n",
    "        output_path = os.path.join(batch_download_dir, filename)\n",
    "        \n",
    "        try:\n",
    "            # Note: download_dataset requires knowing the exact filename\n",
    "            # In practice, you'd first list files in the dataset\n",
    "            client.download_dataset(dataset_id, filename, output_path)\n",
    "            downloaded_files.append(output_path)\n",
    "            print(f\"  ✓ Downloaded: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Failed to download {filename}: {e}\")\n",
    "    \n",
    "    # Create a manifest file with batch information\n",
    "    manifest = {\n",
    "        \"batch_id\": batch_id,\n",
    "        \"download_date\": datetime.now().isoformat(),\n",
    "        \"batch_sample\": batch_info['batch_sample'],\n",
    "        \"child_samples\": batch_info['child_samples'],\n",
    "        \"datasets\": batch_info['datasets'],\n",
    "        \"downloaded_files\": downloaded_files\n",
    "    }\n",
    "    \n",
    "    manifest_path = os.path.join(batch_download_dir, \"batch_manifest.json\")\n",
    "    with open(manifest_path, 'w') as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nDownload complete! Files saved to: {batch_download_dir}\")\n",
    "    print(f\"Manifest created: {manifest_path}\")\n",
    "    \n",
    "    return batch_download_dir\n",
    "\n",
    "# Download all batch data\n",
    "download_dir = download_batch_data(client, batch_info)\n",
    "\n",
    "# List downloaded files\n",
    "print(f\"\\nDownloaded files:\")\n",
    "for file in os.listdir(download_dir):\n",
    "    file_path = os.path.join(download_dir, file)\n",
    "    size = os.path.getsize(file_path)\n",
    "    print(f\"  - {file} ({size} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial demonstrated a complete workflow for batch sample management using pycrucible:\n",
    "\n",
    "1. **✅ Created a batch of samples** using a hierarchical parent-child structure\n",
    "2. **✅ Added a batch-level dataset** (experimental protocol) associated with all samples\n",
    "3. **✅ Created individual datasets** (microscopy images) for each sample in the batch\n",
    "4. **✅ Added custom metadata** to individual samples via their datasets\n",
    "5. **✅ Retrieved and downloaded** all data from the batch using the batch ID\n",
    "\n",
    "### Key Implementation Notes:\n",
    "\n",
    "- **Hierarchical Structure**: Used `parent_sample_id` to link individual samples to a batch sample\n",
    "- **Metadata Storage**: Since samples don't support scientific metadata directly, we stored custom metadata in the associated datasets\n",
    "- **Batch Queries**: Created helper functions to find and manage batch-related data\n",
    "- **Download Strategy**: Implemented batch downloading with manifest files for data provenance\n",
    "\n",
    "### Future Enhancements:\n",
    "\n",
    "To make this workflow more robust, consider extending the pycrucible client with:\n",
    "- Native batch sample creation methods\n",
    "- Query methods to find samples by parent_sample_id\n",
    "- Bulk dataset association methods\n",
    "- Enhanced download methods that auto-discover files in datasets\n",
    "- Sample-level metadata support\n",
    "\n",
    "This approach provides a solid foundation for managing related samples and their associated data in a structured, queryable way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}