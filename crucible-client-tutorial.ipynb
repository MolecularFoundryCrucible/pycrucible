{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crucible Python Client Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the Crucible Python Client to interact with the Crucible Data Platform.\n",
    "<br>\n",
    "You can run ```help(client.{function_name})``` to see details about any of the client functions.\n",
    "<br>\n",
    "Replace ```{function_name}``` with the name of the function you want to see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First, import the client and initialize it with your API credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pycrucible import CrucibleClient\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the client\n",
    "api_url = 'https://crucible.lbl.gov/testapi'\n",
    "api_key = os.environ.get(\"admin_apikey\")  # or \"admin_apikey\" for admin access\n",
    "\n",
    "client = CrucibleClient(api_url, api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Searching for Datasets\n",
    "\n",
    "Use `list_datasets()` to search for datasets with optional filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List datasets \n",
    "datasets = client.list_datasets(limit=1000)\n",
    "print(f\"Found {len(datasets)} datasets\")\n",
    "print(f\"\\nFirst dataset: {datasets[0]['dataset_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(client.list_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Available Filters\n",
    "\n",
    "You can filter datasets using various parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by keyword\n",
    "keyword_datasets = client.list_datasets(keyword='tem', limit=5)\n",
    "print(f\"Datasets with keyword 'tem': {len(keyword_datasets)}\")\n",
    "\n",
    "# Filter by instrument\n",
    "instrument_datasets = client.list_datasets(instrument_name='titanx', limit=5)\n",
    "print(f\"Datasets from 'titanx' instrument: {len(instrument_datasets)}\")\n",
    "\n",
    "# Filter by owner ORCID\n",
    "owner_datasets = client.list_datasets(owner_orcid='0009-0001-9493-2006', limit=5)\n",
    "print(f\"Datasets by owner: {len(owner_datasets)}\")\n",
    "\n",
    "# Combine multiple filters\n",
    "filtered = client.list_datasets(keyword='tem', instrument_name='titanx', limit=5)\n",
    "print(f\"Datasets matching multiple filters: {len(filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search datasets by sample ID\n",
    "sample_id = '0t3q9zq7enrhf0004dvevszkmm'  # Example sample ID\n",
    "sample_datasets = client.list_datasets(sample_id=sample_id)\n",
    "print(f\"Datasets for sample {sample_id}: {len(sample_datasets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search datasets by sample ID and filter\n",
    "sample_id = '0t3q9zq7enrhf0004dvevszkmm'  # Example sample ID\n",
    "sample_datasets = client.list_datasets(sample_id=sample_id, data_format = 'h5')\n",
    "print(f\"Datasets for sample {sample_id}: {len(sample_datasets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Adding Datasets\n",
    "\n",
    "There are two main ways to add datasets: from JSON metadata only, or with a file upload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option A: Add Dataset from JSON (metadata only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with metadata only\n",
    "from pycrucible.models import BaseDataset\n",
    "my_dataset = BaseDataset(dataset_name='TEST3 - My New Dataset',\n",
    "                         owner_orcid='0009-0001-9493-2006',\n",
    "                         project_id='MFP08540',\n",
    "                         instrument_name='titanx',\n",
    "                         measurement='haadf',\n",
    "                         public=False\n",
    "                        )\n",
    "\n",
    "result = client.create_new_dataset(dataset = my_dataset,\n",
    "                                   scientific_metadata={'voltage': '200kV', 'magnification': '50000x'},\n",
    "                                   keywords=['tem', 'nanoparticles']\n",
    "                                   )\n",
    "\n",
    "dsid = result['created_record']['unique_id']\n",
    "print(f\"Created dataset: {dsid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what it looks like in the database\n",
    "new_ds = client.get_dataset(dsid = dsid)\n",
    "new_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option B: Add Dataset with File Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(client.create_new_dataset_from_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsid='0sdazahr0nxh300075jj73j2kg'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CrucibleClient' object has no attribute 'ingest_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     18\u001b[39m file_path = \u001b[33m'\u001b[39m\u001b[33m./test-data/0sdazahr0nxh300075jj73j2kg_240119_144139_hyperspec_picam_mcl.h5\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     19\u001b[39m my_file_dataset = BaseDataset(dataset_name=\u001b[33m'\u001b[39m\u001b[33mTEST4 - Dataset with File\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     20\u001b[39m                               unique_id=\u001b[33m'\u001b[39m\u001b[33m0sdazahr0nxh300075jj73j2kg\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     21\u001b[39m                               owner_orcid=\u001b[33m'\u001b[39m\u001b[33m0009-0001-9493-2006\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     22\u001b[39m                               project_id=\u001b[33m'\u001b[39m\u001b[33mMFP08540\u001b[39m\u001b[33m'\u001b[39m,)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m result = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_new_dataset_from_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmy_file_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfiles_to_upload\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscientific_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnotes\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mthis is a test dataset we keep reusing\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeywords\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mingestor\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mHyperspecScopeFoundryH5Ingestor\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Optional: specify ingestion class\u001b[39;49;00m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwait_for_ingestion_response\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     30\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m dsid = result[\u001b[33m'\u001b[39m\u001b[33mcreated_record\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33munique_id\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreated dataset with file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdsid\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/pycrucible/pycrucible/pycrucible.py:1059\u001b[39m, in \u001b[36mCrucibleClient.create_new_dataset_from_files\u001b[39m\u001b[34m(self, dataset, files_to_upload, scientific_metadata, keywords, get_user_info_function, ingestor, verbose, wait_for_ingestion_response)\u001b[39m\n\u001b[32m   1056\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m   1057\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msubmitting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdsid\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to be ingested from file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmain_file_cloud\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m using the class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mingestor\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1059\u001b[39m ingest_req_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mingest_dataset\u001b[49m(dsid, main_file_cloud, ingestor)\n\u001b[32m   1061\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose: \n\u001b[32m   1062\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mingestion request \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mingest_req_info[\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is added to the queue\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'CrucibleClient' object has no attribute 'ingest_dataset'"
     ]
    }
   ],
   "source": [
    "# Create dataset with file upload and ingestion\n",
    "from pycrucible.models import BaseDataset\n",
    "\n",
    "file_path = './test-data/0sdazahr0nxh300075jj73j2kg_240119_144139_hyperspec_picam_mcl.h5'\n",
    "my_file_dataset = BaseDataset(dataset_name='TEST4 - Dataset with File',\n",
    "                              unique_id='0sdazahr0nxh300075jj73j2kg', \n",
    "                              owner_orcid='0009-0001-9493-2006',\n",
    "                              project_id='MFP08540',)\n",
    "result = client.create_new_dataset_from_files(\n",
    "    dataset = my_file_dataset,\n",
    "    files_to_upload=[file_path],\n",
    "    scientific_metadata={'notes': 'this is a test dataset we keep reusing'},\n",
    "    keywords=['test'],\n",
    "    ingestor='HyperspecScopeFoundryH5Ingestor',  # Optional: specify ingestion class\n",
    "    wait_for_ingestion_response=True\n",
    ")\n",
    "\n",
    "dsid = result['created_record']['unique_id']\n",
    "print(f\"Created dataset with file: {dsid}\")\n",
    "print(f\"Ingestion status: {result['ingestion_request']['status']}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# check what it looks like in the database\n",
    "dsid = result['created_record']['unique_id']\n",
    "new_ds = client.get_dataset(dsid = dsid)\n",
    "new_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Updating Datasets\n",
    "\n",
    "Update existing dataset fields or scientific metadata."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Update basic dataset fields\n",
    "dsid = result['created_record']['unique_id']\n",
    "updated = client.update_dataset(\n",
    "    dsid,\n",
    "    dataset_name='Updated Dataset Name',\n",
    "    public=True,\n",
    "    measurement='Hyperspectral Raman'\n",
    ")\n",
    "\n",
    "print(f\"Updated dataset: {updated['dataset_name']}\")\n",
    "print(f\"Now public: {updated['public']}\")\n",
    "client.get_dataset(dsid = dsid)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Update scientific metadata (merge with existing)\n",
    "dsid = result['created_record']['unique_id']\n",
    "new_metadata = {\n",
    "    'new_parameter': 'new_value',\n",
    "    'analysis_date': '2024-01-15'\n",
    "}\n",
    "\n",
    "client.update_scientific_metadata(dsid, new_metadata, overwrite=False)\n",
    "print(\"Scientific metadata updated (merged)\")\n",
    "client.get_dataset(dsid = dsid, include_metadata = True)\n",
    "# Or overwrite all scientific metadata\n",
    "complete_metadata = {\n",
    "    'voltage': '300kV',\n",
    "    'magnification': '100000x'\n",
    "}\n",
    "\n",
    "client.update_scientific_metadata(dsid, complete_metadata, overwrite=True)\n",
    "print(\"Scientific metadata replaced\")\n",
    "client.get_dataset(dsid = dsid, include_metadata = True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add keywords to a dataset\n",
    "client.add_dataset_keyword(dsid, 'nanomaterials')\n",
    "client.add_dataset_keyword(dsid, 'characterization')\n",
    "\n",
    "print(\"Keywords added\")\n",
    "client.get_dataset_keywords(dsid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Downloading Datasets\n",
    "\n",
    "Download dataset files to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset (uses file_to_upload field from dataset)\n",
    "import time\n",
    "st = time.time()\n",
    "dsid = '0swxg30hgxtdb000wxqmapw6kr'\n",
    "\n",
    "client.download_dataset(dsid)\n",
    "end = time.time()\n",
    "print(end - st)\n",
    "print(f\"Dataset downloaded to: crucible-downloads/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download with custom output path\n",
    "dsid = '04qed8jsxd3avcgk7d443rw7t4'\n",
    "client.download_dataset(\n",
    "    dsid,\n",
    "    output_path='./experiment1_data/'\n",
    ")\n",
    "print(\"Downloaded to custom location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Working with Samples\n",
    "\n",
    "Create samples and link them to datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a New Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mfid\n",
    "\n",
    "# Create a new sample\n",
    "sample = client.add_sample(\n",
    "    unique_id=mfid.mfid()[0],\n",
    "    sample_name='TEST00000000',\n",
    "    description='Au Nanoparticles Batch 42',\n",
    "    owner_orcid='0009-0001-9493-2006',\n",
    "    creation_date='2024-01-15'\n",
    ")\n",
    "\n",
    "sample_id = sample['unique_id']\n",
    "print(f\"Created sample: {sample_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get_sample(sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link Sample to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link an existing sample to a dataset\n",
    "dataset_id = '0sdazahr0nxh300075jj73j2kg'\n",
    "sample_id = sample_id\n",
    "\n",
    "link = client.add_sample_to_dataset(dataset_id, sample_id)\n",
    "print(f\"Linked sample {sample_id} to dataset {dataset_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get_sample(sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Datasets for a Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all datasets associated with a sample\n",
    "sample_datasets = client.list_datasets(sample_id=sample_id)\n",
    "print(f\"Found {len(sample_datasets)} datasets for sample {sample_id}\")\n",
    "\n",
    "for ds in sample_datasets[:3]:  # Show first 3\n",
    "    print(f\"  - {ds['dataset_name']} ({ds['unique_id']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Sample Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve sample details\n",
    "sample = client.get_sample(sample_id)\n",
    "print(f\"Sample name: {sample['sample_name']}\")\n",
    "print(f\"Description: {sample.get('description', 'N/A')}\")\n",
    "print(f\"Owner ORCID: {sample.get('owner_orcid', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get full dataset information with metadata\n",
    "dataset = client.get_dataset(dsid, include_metadata=True)\n",
    "print(f\"Dataset: {dataset['dataset_name']}\")\n",
    "print(f\"Instrument: {dataset.get('instrument_name', 'N/A')}\")\n",
    "print(f\"Scientific metadata: {dataset.get('scientific_metadata', {})}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available projects \n",
    "projects = client.list_projects(limit=5)\n",
    "for proj in projects:\n",
    "    print(f\"{proj['project_id']}: {proj.get('title', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available instruments\n",
    "instruments = client.list_instruments(limit=5)\n",
    "for inst in instruments:\n",
    "    print(f\"{inst['instrument_name']} (Location: {inst.get('location', 'N/A')})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(client.list_instruments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycruc",
   "language": "python",
   "name": "pycruc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
